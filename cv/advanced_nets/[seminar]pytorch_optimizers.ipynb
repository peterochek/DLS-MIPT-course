{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"x4kC-U8dOpZU"},"outputs":[],"source":["from __future__ import print_function, division\n","\n","import matplotlib.pyplot as plt\n","\n","import torch\n","from torch import nn, optim\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets\n","\n","from tqdm import trange"]},{"cell_type":"markdown","metadata":{"id":"j-U2A8nY6c6C"},"source":["# Загрузка датасета\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1YS1-h9e6c6D","scrolled":true},"outputs":[],"source":["classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","\n","def get_dataloaders(batch_size):\n","    transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","    \n","    trainset = datasets.CIFAR10(root='./data', train=True,\n","                                            download=True, transform=transform)\n","    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                              shuffle=True, num_workers=2)\n","    testset = datasets.CIFAR10(root='./data', train=False,\n","                                           download=True, transform=transform)\n","    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                             shuffle=False, num_workers=2)\n","    return trainloader, testloader"]},{"cell_type":"markdown","metadata":{"id":"OW14I7DorFEf"},"source":["В PyTorch датасетом считается любой объект, для которого определены методы `__len__(self)` и `__getitem__(self, i)`."]},{"cell_type":"markdown","metadata":{"id":"c5prjwEt6c6E"},"source":["# Код обучения"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SYG6fleW6c6E"},"outputs":[],"source":["def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n","    train_losses = []\n","    val_losses = []\n","    valid_accuracies = []\n","    for epoch in trange(epochs):\n","        model.train()\n","        loss_sum = 0\n","        for xb, yb in train_dl:\n","            xb, yb = xb, yb\n","            loss = loss_func(model(xb), yb)\n","            loss_sum += loss.item()\n","            \n","            loss.backward()\n","            opt.step()\n","            opt.zero_grad()\n","        train_losses.append(loss_sum / len(train_dl))\n","\n","        model.eval()\n","        loss_sum = 0\n","        correct = 0\n","        num = 0\n","        with torch.no_grad():\n","            for xb, yb in valid_dl:\n","                xb, yb = xb, yb\n","                probs = model(xb)\n","                loss_sum += loss_func(probs, yb).item()\n","                \n","                _, preds = torch.max(probs, axis=-1)\n","                correct += (preds == yb).sum().item()\n","                num += len(xb)\n","                \n","        val_losses.append(loss_sum / len(valid_dl))\n","        valid_accuracies.append(correct / num)\n","        \n","    return train_losses, val_losses, valid_accuracies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v4rX2RGI6c6F"},"outputs":[],"source":["def plot_trainig(train_losses, valid_losses, valid_accuracies):\n","    plt.figure(figsize=(12, 9))\n","    plt.subplot(2, 1, 1)\n","    plt.xlabel('epoch')\n","    plt.plot(train_losses, label='train_loss')\n","    plt.plot(valid_losses, label='valid_loss')\n","    plt.legend()\n","    \n","    plt.subplot(2, 1, 2)\n","    plt.xlabel('epoch')\n","    plt.plot(valid_accuracies, label='valid accuracy')\n","    plt.legend()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s_PetTRd6c6F"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n","        self.conv1 = nn.Conv2d(3, 6, 3)\n","        self.conv2 = nn.Conv2d(6, 16, 3)\n","        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 5x5 image dimension\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","\n","    def forward(self, x):\n","        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n","        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n","        x = x.view(x.shape[0], -1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["b57dea5b9be64d28bffa55464dd942f8","d63cfdc95cc144068ba757b2c95583ad","aacecde53df249d5b0139b3f92bf71b1","f6a3054f41ae4beaae464c7d28a59cca","1b046762e6404d61a12afdabdec0c162","b97ba63eb3ac451d9aa9ffc03c61df2a","dafe92740c5b4460a513ce3ed344694b","c6f4db06c1264b17a397f966b9ad5a6c","41b163d6714b47318e4ea9d985203cdd","5a63fd8163674fd1a0df2db288d4d866","0eca8004360e4f378f04917505bd6701","c524963f867b4e4197ddb8f28c15f6b8","a06891e0c9b341468d130685d489f1fa","fa869db7986c45a69df8740a7b845517","b2151f4af803472199dd95a00fea0c2e","3e318e60815e419badfaa0063ad65a41","680aa74ba15b4bba833c063d8c40c057","f9a19d3b09bd43f39411b3ec2fa4189f","a552b3c70b4f489cb49011a2c964e10c","719e38cd01db49acae78bbe273cab23e"]},"id":"XZkhm7n46c6F","outputId":"445a0117-4a21-4632-c085-4844403e0b77"},"outputs":[],"source":["model = Model()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","info = fit(10, model, criterion, optimizer, *get_dataloaders(4))\n","plot_trainig(*info)"]},{"cell_type":"markdown","metadata":{"id":"TuTwMGGS6c6G"},"source":["# Adam\n","\n","Это самый популярный оптимизатор для нейронных сетей. Если вам интересны сравнения разных алгоритмов, то можете почитать эту статью https://arxiv.org/abs/1609.04747."]},{"cell_type":"markdown","metadata":{"id":"GO2wjG5u6c6H"},"source":["### Adam, применение в PyTorch"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["d546d03d1b4945e381be58c274a81813","2560f184d0ea435690d0f86477da2fc4","79ce58e474f24872a85ac98be2744b56","7af660db7a8d4077bf54c4b81177503f","c4761a908eb94380bcf592e2cf767e39","d73bfba11a1a4e2787b2cb139a6676d9","eec9dfb9c7bf48878be130d478eefbf5","28e1c44cd1bf499687394f795818157a","5ebdba91c88249fa9d86ac973f8ff85b","feb6e28250fb445499d474812efb09e0","d0ea00ca5ad4474d971b6d72a6f08847","686114e4c066488e9d4c0b5781f7e23f","8c5914e2e7b94a2bad5374017af46e8c","159c62cd61ef498b81f0416e26990b01","4ade97cdbf6b461ba0c5b8804fcc47ae","521601ad0f46451687bfe576fe88b1c1","1bf59d210edc4d9da9252e5073090820","42e8d1e886be4954a57b6c477913faca","19f8f357ca34409985f92cef67a20e38","fbff08758f17474c9b8a1714b4f65af2"]},"id":"jxmBiXio6c6H","outputId":"7eb6969c-2579-407a-a6bb-0dc1f23341d4","scrolled":true},"outputs":[],"source":["model = Model()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n","\n","info = fit(10, model, criterion, optimizer, *get_dataloaders(4))\n","plot_trainig(*info)"]},{"cell_type":"markdown","metadata":{"id":"_INYsiAc6c6I"},"source":["# Weight Decay\n","\n","Для регуляризации линейных моделей мы прибавляли к лоссу сумму квадратов весов, умноженных на некоторый коэффициент:\n","$$L(\\mathbf{w})=\\sum_{i=1}^{l}\\left(\\mathbf{x}_{i}^{T} \\mathbf{w}-y_{i}\\right)^{2}+\\beta \\sum_{j=1}^{n} w_{j}^{2}$$\n","\n","Для нейронных сетей мы можем выбрать такую же реугляризацию. Она называется WeightDecay. Во многие оптимизаторы можно передать параметр `weight_decay` и он будет являться коэффициентом, на который домножается сумма квадратовв весов.\n","\n","Обычно используют weight_decay=0.01 или 0.005."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["b8836c538efc4524ab4d97b85a822bdc","a4d8a86e6f6a4afcb39be510f898d977"]},"id":"a6BPAnNh6c6J","outputId":"7d169feb-3e5b-4657-e20e-41967553c678"},"outputs":[],"source":["model = Model()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.01)\n","\n","info = fit(10, model, criterion, optimizer, *get_dataloaders(4))\n","plot_trainig(*info)"]},{"cell_type":"markdown","metadata":{"id":"pIuBGC8-6c6K"},"source":["# LR scheduling\n","\n","Часто мы хотим, чтобы наш learning rate как-то изменялся во время обучения. Стратегия, по которой мы будем изменять lr называется lr scheduilng.\n","\n","Например, мы можем хотеть, чтобы learning_rate уменьшался с каждой эпохой в фиксированное число раз. Тогда в начале мы будем быстро двигаться к минимуму, а в конце точно не промахнемся мимо него за счет малых шагов. Такая стратегия называется LR Decay.\n","![](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/images/lr2.png)\n","\n","(Слева используется lr decay, справа нет. Слева мы можем не подбирать идеально точно lr и все равно со временем сойтись.)"]},{"cell_type":"markdown","metadata":{"id":"G9UTIq3z6c6L"},"source":["Нам нужно немного изменить train loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3Y4hl426c6M"},"outputs":[],"source":["def fit(epochs, model, loss_func, opt, train_dl, valid_dl, lr_sched=None):\n","    train_losses = []\n","    val_losses = []\n","    valid_accuracies = []\n","    for epoch in trange(epochs):\n","        model.train()\n","        loss_sum = 0\n","        for xb, yb in train_dl:\n","            xb, yb = xb, yb\n","            loss = loss_func(model(xb), yb)\n","            loss_sum += loss.item()\n","            \n","            loss.backward()\n","            opt.step()\n","            opt.zero_grad()\n","        train_losses.append(loss_sum / len(train_dl))\n","\n","        model.eval()\n","        loss_sum = 0\n","        correct = 0\n","        num = 0\n","        with torch.no_grad():\n","            for xb, yb in valid_dl:\n","                xb, yb = xb, yb\n","                probs = model(xb)\n","                loss_sum += loss_func(probs, yb).item()\n","                \n","                _, preds = torch.max(probs, axis=-1)\n","                correct += (preds == yb).sum().item()\n","                num += len(xb)\n","                \n","        val_losses.append(loss_sum / len(valid_dl))\n","        valid_accuracies.append(correct / num)\n","        \n","        # CHANGES HERE\n","        lr_ched.step()\n","        # CHANGES END\n","        \n","    return train_losses, val_losses, valid_accuracies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlUdcLkI6c6N"},"outputs":[],"source":["model = Model()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=0.01)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer=my_optim, gamma=0.5)\n","\n","info = fit(10, model, criterion, optimizer, *get_dataloaders(4))\n","plot_trainig(*info)"]},{"cell_type":"markdown","metadata":{"id":"oOOKYQOI6c6N"},"source":["# Чекпоинтинг.\n","\n","Есть очень хорошее официальное руководство: https://pytorch.org/tutorials/beginner/saving_loading_models.html."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"[seminar]pytorch_optimizers.ipynb","provenance":[]},"interpreter":{"hash":"5eb2e0c23f8e38f19a3cfe8ad2d7bbb895a86b1e106b247f2b169180d03d2047"},"kernelspec":{"display_name":"Python 3.8.12 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":0}
