{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR1HzizaCVhF"
      },
      "source": [
        "<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1I8kDikouqpH4hf7JBiSYAeNT2IO52T-T\" width=600 height=480/></p>\n",
        "<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n",
        "\n",
        "<h3 style=\"text-align: center;\"><b>Семинар. Весна 2021</b></h3>\n",
        "\n",
        "# Autoencoders. Часть 2\n",
        "## Variational autoencoders (VAE)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rYVufslYY7D"
      },
      "source": [
        "В этом ноутбуке мы будем тренировать автоэнкодеры кодировать лица людей. Для этого возьмем датасет \"Labeled Faces in the Wild\" (LFW) (http://vis-www.cs.umass.edu/lfw/). Код для скачивания и загрузки датасета написан в ячейках ниже"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zhAhxVVGJyEP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "import skimage.io\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nclDyb9ACw5O"
      },
      "outputs": [],
      "source": [
        "def fetch_dataset(images_name = \"lfw-deepfunneled\",\n",
        "                      dx=80,dy=80,\n",
        "                      dimx=64,dimy=64\n",
        "    ):\n",
        "\n",
        "    #read attrs\n",
        "    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,) \n",
        "    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n",
        "\n",
        "\n",
        "    #read photos\n",
        "    photo_ids = []\n",
        "    for dirpath, dirnames, filenames in os.walk(images_name):\n",
        "        for fname in filenames:\n",
        "            if fname.endswith(\".jpg\"):\n",
        "                fpath = os.path.join(dirpath,fname)\n",
        "                photo_id = fname[:-4].replace('_',' ').split()\n",
        "                person_id = ' '.join(photo_id[:-1])\n",
        "                photo_number = int(photo_id[-1])\n",
        "                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
        "\n",
        "    photo_ids = pd.DataFrame(photo_ids)\n",
        "    # print(photo_ids)\n",
        "    #mass-merge\n",
        "    #(photos now have same order as attributes)\n",
        "    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n",
        "\n",
        "    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n",
        "\n",
        "    # print(df.shape)\n",
        "    #image preprocessing\n",
        "    all_photos =df['photo_path'].apply(skimage.io.imread)\\\n",
        "                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n",
        "                                .apply(lambda img: resize(img,[dimx,dimy]))\n",
        "\n",
        "    all_photos = np.stack(all_photos.values)#.astype('uint8')\n",
        "    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n",
        "    \n",
        "    return all_photos,all_attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9waIon7C3ne",
        "outputId": "316203a5-2d11-4429-a13f-152f0201efb7"
      },
      "outputs": [],
      "source": [
        "all_photos, all_attrs = fetch_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q78_PAMLC9wh"
      },
      "source": [
        "Давайте построим простейший VAE, который будет состоять из линейных слоев. Обратите особое внимание на метод `reparameterize`. Это так называемый **reparametrization trick**: преобразование, позволяющее перейти от случайной величины, имеющей стандартное нормальное распределение (со средним 0 и дисперсией 1), к произвольной нормальной случайной величине. Такой трюк позволяет нам генерировать латентный вектор из произвольного нормального распределения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iWhj4_5GwwXc"
      },
      "outputs": [],
      "source": [
        "features = 16\n",
        "# define a simple linear VAE\n",
        "class LinearVAE(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LinearVAE, self).__init__()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        " \n",
        "        # encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_features=12288, out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=512, out_features=features*2)\n",
        "        )\n",
        " \n",
        "        # decoder \n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(in_features=features, out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=512, out_features=12288)\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, log_var):\n",
        "        \"\"\"\n",
        "        :param mu: mean from the encoder's latent space\n",
        "        :param log_var: log variance from the encoder's latent space\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * log_var) # standard deviation\n",
        "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
        "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
        "        return sample\n",
        " \n",
        "    def forward(self, x):\n",
        "        # encoding\n",
        "        x = self.flatten(x).float()\n",
        "        x = self.encoder(x).view(-1, 2, features)\n",
        "        # get `mu` and `log_var`\n",
        "        mu = x[:, 0, :] # the first feature values as mean\n",
        "        log_var = x[:, 1, :] # the other feature values as variance\n",
        "        # get the latent vector through reparameterization\n",
        "        z = self.reparameterize(mu, log_var)\n",
        " \n",
        "        # decoding\n",
        "        x = self.decoder(z)\n",
        "        reconstruction = torch.sigmoid(x)\n",
        "        return reconstruction, mu, log_var\n",
        "\n",
        "    def sample(self, z):\n",
        "        generated = self.decoder(z)\n",
        "        generated = torch.sigmoid(generated)\n",
        "        generated = generated.view(-1, 64, 64, 3)\n",
        "        return generated\n",
        "\n",
        "    def get_latent_vector(self, x):\n",
        "        x = self.flatten(x).float()\n",
        "        x = self.encoder(x).view(-1, 2, features)\n",
        "        # get `mu` and `log_var`\n",
        "        mu = x[:, 0, :] # the first feature values as mean\n",
        "        log_var = x[:, 1, :] # the other feature values as variance\n",
        "        # get the latent vector through reparameterization\n",
        "        z = self.reparameterize(mu, log_var)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAB77d-PYY76"
      },
      "source": [
        "Определим лосс и его компоненты для VAE:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxJrkXGQo5bp"
      },
      "source": [
        "Из лекций вы узнали, что лосс у VAE состоит из двух частей: KL-дивергенции, отвечающей за то, как далеко друг от друга отстоят распределения, соответствующие разным лицам, и log-likelihood, отвечающий за то, насколько хорошо мы кодируем исходное изображение.\n",
        "Общий лосс будет выглядеть так:\n",
        "\n",
        "$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n",
        "\n",
        "Формула для KL-дивергенции:\n",
        "\n",
        "$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n",
        "\n",
        "В качестве log-likelihood возьмем привычную нам кросс-энтропию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ac5ey7uIYY77"
      },
      "outputs": [],
      "source": [
        "def KL_divergence(mu, logsigma):\n",
        "    \"\"\"\n",
        "    часть функции потерь, которая отвечает за \"близость\" латентных представлений разных людей\n",
        "    \"\"\"\n",
        "    loss = -0.5 * torch.sum(1 + logsigma - mu.pow(2) - logsigma.exp())\n",
        "    return loss\n",
        "\n",
        "def log_likelihood(x, reconstruction):\n",
        "    \"\"\"\n",
        "    часть функции потерь, которая отвечает за качество реконструкции\n",
        "    \"\"\"\n",
        "    loss = nn.BCELoss(reduction='sum')\n",
        "    return loss(reconstruction, x)\n",
        "\n",
        "def loss_vae(x, mu, logsigma, reconstruction):\n",
        "    return KL_divergence(mu, logsigma) + log_likelihood(x, reconstruction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPJQu70eYY79"
      },
      "source": [
        "И обучим модель:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dtCjfqXdYY79"
      },
      "outputs": [],
      "source": [
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "criterion = loss_vae\n",
        "\n",
        "autoencoder = LinearVAE().to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(autoencoder.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kq9PkQn-XzVO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 64, 64, 3])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_photos, val_photos, train_attrs, val_attrs = train_test_split(all_photos, all_attrs,\n",
        "                                                                    train_size=0.9, shuffle=False)\n",
        "train_loader = torch.utils.data.DataLoader(train_photos, batch_size=32)\n",
        "val_loader = torch.utils.data.DataLoader(val_photos, batch_size=32)\n",
        "\n",
        "next(iter(train_loader)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "f954530e0bab405db5e50802f5f7db69",
            "564c4d8949f14ad6912571e7a24b965c",
            "5152aca58d6f4ab786c4a79c01a3ab57",
            "aeef5860ba8e413292b69a97a85e738c",
            "0536d6f9ca8649678ab8ec3b17e365e7",
            "54e0913651f04401909d31346e409b4e",
            "0b1efd49dc94443aa3e713ccfb3eeb05",
            "bfab8425bd9e4f8d98cc06cbd7343c0c"
          ]
        },
        "id": "pqtmSI3QXWZl",
        "outputId": "13b7d969-21c2-4bbe-8996-dd688c445fc0"
      },
      "outputs": [],
      "source": [
        "n_epochs = 50\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in tqdm(range(n_epochs)):\n",
        "    autoencoder.train()\n",
        "    train_losses_per_epoch = []\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        reconstruction, mu, logsigma = autoencoder(batch.to(device))\n",
        "        reconstruction = reconstruction.view(-1, 64, 64, 3)\n",
        "        loss = criterion(batch.to(device).float(), mu, logsigma, reconstruction)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_losses_per_epoch.append(loss.item())\n",
        "\n",
        "    train_losses.append(np.mean(train_losses_per_epoch))\n",
        "\n",
        "    autoencoder.eval()\n",
        "    val_losses_per_epoch = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "          reconstruction, mu, logsigma = autoencoder(batch.to(device))\n",
        "          reconstruction = reconstruction.view(-1, 64, 64, 3)\n",
        "          loss = criterion(batch.to(device).float(), mu, logsigma, reconstruction)\n",
        "          val_losses_per_epoch.append(loss.item())\n",
        "\n",
        "    val_losses.append(np.mean(val_losses_per_epoch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkxW_8fkYY8B",
        "scrolled": true
      },
      "source": [
        "Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Jd3BWM_YY8C"
      },
      "outputs": [],
      "source": [
        "autoencoder.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "      reconstruction, mu, logsigma = autoencoder(batch.to(device))\n",
        "      reconstruction = reconstruction.view(-1, 64, 64, 3)\n",
        "      result = reconstruction.cpu().detach().numpy()\n",
        "      ground_truth = batch.numpy()\n",
        "      break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QDXsgUeo94IV",
        "outputId": "77e4874e-5ff8-4150-ea0d-088052d003bf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 20))\n",
        "for i, (gt, res) in enumerate(zip(ground_truth[:5], result[:5])):\n",
        "  plt.subplot(5, 2, 2*i+1)\n",
        "  plt.imshow(gt)\n",
        "  plt.subplot(5, 2, 2*i+2)\n",
        "  plt.imshow(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0jI-FKEYY8E"
      },
      "source": [
        "Давайте теперь попробуем заняться семплированием из нашей VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po-KV8E8YY8F"
      },
      "source": [
        "## Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQXYIXjoYY8F"
      },
      "source": [
        "Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOhhH-osYY8G"
      },
      "outputs": [],
      "source": [
        "# вспомните про замечание из этого же пункта обычного AE про распределение латентных переменных\n",
        "z = np.array([np.random.normal(0, 1, 16) for i in range(10)])\n",
        "output = autoencoder.sample(torch.FloatTensor(z).to(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2F7jnnI-EbNt",
        "outputId": "29cb3a11-6d2b-471b-e9c5-8336a9658949"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 18))\n",
        "for i in range(output.shape[0]):\n",
        "  plt.subplot(output.shape[0] // 2, 2, i + 1)\n",
        "  generated = output[i].cpu().detach().numpy()\n",
        "  plt.imshow(generated)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vy3GZjiLQQQ"
      },
      "source": [
        "## Еще немного экспериментов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvT104NkMcVq"
      },
      "source": [
        "Сгенерируем латентные векторы для двух изображений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuUDzK08ILrX"
      },
      "outputs": [],
      "source": [
        "gt_0 = torch.FloatTensor([ground_truth[0]]).to(device)\n",
        "gt_1 = torch.FloatTensor([ground_truth[1]]).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkrqUWfeISFo"
      },
      "outputs": [],
      "source": [
        "first_latent_vector = autoencoder.get_latent_vector(gt_0)\n",
        "second_latent_vector = autoencoder.get_latent_vector(gt_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "luG-4kkzJSs4",
        "outputId": "05e4c142-06bc-4a61-a38a-0cfac0242a3f"
      },
      "outputs": [],
      "source": [
        "plt.imshow(autoencoder.sample(first_latent_vector)[0].cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "UmHJYjKrKhnv",
        "outputId": "08d77d4a-3533-4063-a7bc-5c5e8f39861a"
      },
      "outputs": [],
      "source": [
        "plt.imshow(autoencoder.sample(second_latent_vector)[0].cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS4Ns1EKmDsH"
      },
      "source": [
        "Теперь посмотрим, как будет выглядеть их *выпуклая комбинация*: $\\alpha l_1 + (1 - \\alpha)l_2$, где $l_1, l_2$ – латентные векторы, $\\alpha \\in [0, 1]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "GVUJFTjxKrwE",
        "outputId": "02eb11a4-7a82-4a7a-cc60-5de5ddb40e4b"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(18, 5))\n",
        "for i, alpha in enumerate(np.linspace(0., 1., 5)):\n",
        "  plt.subplot(1, 5, i + 1)\n",
        "  latent = (1 - alpha) * first_latent_vector + alpha * second_latent_vector\n",
        "  img = autoencoder.sample(latent)[0].cpu().detach().numpy()\n",
        "  plt.imshow(img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[seminar]vae.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0536d6f9ca8649678ab8ec3b17e365e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "0b1efd49dc94443aa3e713ccfb3eeb05": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5152aca58d6f4ab786c4a79c01a3ab57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e0913651f04401909d31346e409b4e",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0536d6f9ca8649678ab8ec3b17e365e7",
            "value": 50
          }
        },
        "54e0913651f04401909d31346e409b4e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "564c4d8949f14ad6912571e7a24b965c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aeef5860ba8e413292b69a97a85e738c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfab8425bd9e4f8d98cc06cbd7343c0c",
            "placeholder": "​",
            "style": "IPY_MODEL_0b1efd49dc94443aa3e713ccfb3eeb05",
            "value": " 50/50 [02:51&lt;00:00,  3.43s/it]"
          }
        },
        "bfab8425bd9e4f8d98cc06cbd7343c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f954530e0bab405db5e50802f5f7db69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5152aca58d6f4ab786c4a79c01a3ab57",
              "IPY_MODEL_aeef5860ba8e413292b69a97a85e738c"
            ],
            "layout": "IPY_MODEL_564c4d8949f14ad6912571e7a24b965c"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
