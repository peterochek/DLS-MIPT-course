{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wru2LNFuL2Iq"
   },
   "source": [
    "# Часть 1. Vanilla Autoencoder (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr3STtdpYY7G"
   },
   "source": [
    "## 1.1. Подготовка данных (0.5 балла)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xTNi9JLRYY7I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "73lg3bI2YY7m"
   },
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESCALE_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,) \n",
    "df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n",
    "\n",
    "#read photos\n",
    "photo_ids = []\n",
    "for dirpath, dirnames, filenames in os.walk('lfw-deepfunneled'):\n",
    "    for fname in filenames:\n",
    "        if fname.endswith(\".jpg\"):\n",
    "            fpath = os.path.join(dirpath,fname)\n",
    "            photo_id = fname[:-4].replace('_',' ').split()\n",
    "            person_id = ' '.join(photo_id[:-1])\n",
    "            photo_number = int(photo_id[-1])\n",
    "            photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n",
    "\n",
    "photo_ids = pd.DataFrame(photo_ids)\n",
    "\n",
    "df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n",
    "\n",
    "all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, paths, transform):\n",
    "        self.transform = transform\n",
    "        self.paths = paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_loc = self.paths[idx]\n",
    "        image = Image.open(img_loc).convert(\"RGB\")\n",
    "        tensor_image = self.transform(image)\n",
    "        return tensor_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.CenterCrop(size=(RESCALE_SIZE, RESCALE_SIZE)),\n",
    "                transforms.RandomRotation(degrees=10),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor()\n",
    "            ]),\n",
    "            'val_test': transforms.Compose([\n",
    "                transforms.Resize(size=(RESCALE_SIZE, RESCALE_SIZE)),\n",
    "                transforms.ToTensor()\n",
    "            ]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MSzXXGoYY7X"
   },
   "source": [
    "\n",
    "Разбейте выборку картинок на train и val, выведите несколько картинок в output, чтобы посмотреть, как они выглядят, и приведите картинки к тензорам pytorch, чтобы можно было скормить их сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = df.photo_path.values\n",
    "\n",
    "my_dataset = CustomDataSet(paths, transform=transform['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 0.2\n",
    "\n",
    "val_size = int(len(my_dataset) * val)\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(my_dataset, [len(my_dataset) - val_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9CC-DUhYY7i"
   },
   "source": [
    "## 1.2. Архитектура модели (1.5 балла)\n",
    "В этом разделе мы напишем и обучем обычный автоэнкодер.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b8adf79-8e6a-4b7d-9061-8617a00edbb1%2F__2021-04-30__14.53.33.png?table=block&id=56f187b4-279f-4208-b1ed-4bda5f91bfc0&width=2880&userId=3b1b5e32-1cfb-4b0f-8705-5a524a8f56e3&cache=v2\" alt=\"Autoencoder\">\n",
    "\n",
    "\n",
    "^ напомню, что автоэнкодер выглядит вот так"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "csrNCYh-YY7j"
   },
   "outputs": [],
   "source": [
    "dim_code = 256 # выберите размер латентного вектора"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fjr-N8AWee-k"
   },
   "source": [
    "Реализуем autoencoder. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Экспериментируйте!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8SjHNX-rYY7k"
   },
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, in_channels, latent_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "                \n",
    "        modules = []\n",
    "        hidden_dims = [32, 64, 128, 256, 512, 1024]\n",
    "\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size=3, stride=2, padding=1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        \n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1] * 4)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i+1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride=2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i+1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        \n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=1,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels=3,\n",
    "                                      kernel_size=3, padding=1),\n",
    "                            nn.Tanh())\n",
    "        \n",
    "    def encode(self, input):\n",
    "        result = self.encoder(input)\n",
    "        \n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "        \n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return mu, log_var\n",
    "    \n",
    "    def decode(self, z):\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, 1024, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    " \n",
    "    \n",
    "    def get_latent_vector(self, mu, log_var):\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return z\n",
    " \n",
    "    def forward(self, input):\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        reconstruction = torch.sigmoid(self.decode(z))\n",
    "        \n",
    "        return reconstruction, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(mu, logsigma):\n",
    "    \"\"\"\n",
    "    часть функции потерь, которая отвечает за \"близость\" латентных представлений разных людей\n",
    "    \"\"\"\n",
    "    loss = -0.5 * torch.sum(1 + logsigma - mu.pow(2) - logsigma.exp())\n",
    "    return loss\n",
    "\n",
    "def log_likelihood(x, reconstruction):\n",
    "    \"\"\"\n",
    "    часть функции потерь, которая отвечает за качество реконструкции\n",
    "    \"\"\"\n",
    "    loss = nn.BCELoss(reduction='sum')\n",
    "    return loss(reconstruction, x)\n",
    "\n",
    "def loss_vae(x, mu, logsigma, reconstruction):\n",
    "    return KL_divergence(mu, logsigma) + log_likelihood(x, reconstruction)\n",
    "\n",
    "criterion = loss_vae\n",
    "\n",
    "autoencoder = Autoencoder(3, dim_code).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpntmZCe5L6i"
   },
   "source": [
    "## 1.3 Обучение (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bdxg_3WJYY7o"
   },
   "source": [
    "Осталось написать код обучения автоэнкодера. При этом было бы неплохо в процессе иногда смотреть, как автоэнкодер реконструирует изображения на данном этапе обучения. Наример, после каждой эпохи (прогона train выборки через автоэекодер) можно смотреть, какие реконструкции получились для каких-то изображений val выборки.\n",
    "\n",
    "А, ну еще было бы неплохо выводить графики train и val лоссов в процессе тренировки =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "3H3DOojrYY7o",
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cab3fc0d55fc44a6ad6c1c2e200bbe60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8534e84323d146b3862ed5c3d8ed78e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4deea423d8ce4dd7acdf8b32d4266752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/165 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20448/3909683633.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[0mautoencoder\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[0mtrain_losses_per_epoch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 10\u001B[1;33m     \u001B[1;32mfor\u001B[0m \u001B[0mbatch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtqdm_notebook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     11\u001B[0m         \u001B[0mbatch\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mbatch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\notebook.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    255\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__iter__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    256\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 257\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtqdm_notebook\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__iter__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    258\u001B[0m                 \u001B[1;31m# return super(tqdm...) will not catch exception\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    259\u001B[0m                 \u001B[1;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\tqdm\\std.py\u001B[0m in \u001B[0;36m__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1178\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1179\u001B[0m         \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1180\u001B[1;33m             \u001B[1;32mfor\u001B[0m \u001B[0mobj\u001B[0m \u001B[1;32min\u001B[0m \u001B[0miterable\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1181\u001B[0m                 \u001B[1;32myield\u001B[0m \u001B[0mobj\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1182\u001B[0m                 \u001B[1;31m# Update and possibly print the progressbar.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    519\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    520\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 521\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    522\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    523\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[1;33m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[1;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    559\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_next_data\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    560\u001B[0m         \u001B[0mindex\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 561\u001B[1;33m         \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# may raise StopIteration\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    562\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    563\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     48\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 49\u001B[1;33m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     50\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     51\u001B[0m             \u001B[0mdata\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m    361\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    362\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindices\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 363\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindices\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    364\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    365\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_20448/2173462742.py\u001B[0m in \u001B[0;36m__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m         \u001B[0mimg_loc\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpaths\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 11\u001B[1;33m         \u001B[0mimage\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mImage\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimg_loc\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mconvert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"RGB\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     12\u001B[0m         \u001B[0mtensor_image\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mtensor_image\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001B[0m in \u001B[0;36mconvert\u001B[1;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[0;32m    913\u001B[0m         \"\"\"\n\u001B[0;32m    914\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 915\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    916\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    917\u001B[0m         \u001B[0mhas_transparency\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mget\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"transparency\"\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\ImageFile.py\u001B[0m in \u001B[0;36mload\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    235\u001B[0m                         \u001B[1;32mwhile\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    236\u001B[0m                             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 237\u001B[1;33m                                 \u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdecodermaxblock\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    238\u001B[0m                             \u001B[1;32mexcept\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mIndexError\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstruct\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merror\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    239\u001B[0m                                 \u001B[1;31m# truncated png/gif\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\JpegImagePlugin.py\u001B[0m in \u001B[0;36mload_read\u001B[1;34m(self, read_bytes)\u001B[0m\n\u001B[0;32m    400\u001B[0m         \u001B[0mso\u001B[0m \u001B[0mlibjpeg\u001B[0m \u001B[0mcan\u001B[0m \u001B[0mfinish\u001B[0m \u001B[0mdecoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    401\u001B[0m         \"\"\"\n\u001B[1;32m--> 402\u001B[1;33m         \u001B[0ms\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mread_bytes\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    403\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    404\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0ms\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mImageFile\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mLOAD_TRUNCATED_IMAGES\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import trange, tqdm_notebook\n",
    "\n",
    "n_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in trange(n_epochs):\n",
    "    autoencoder.train()\n",
    "    train_losses_per_epoch = []\n",
    "    for batch in tqdm_notebook(train_loader):\n",
    "        batch = batch.float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, log_var = autoencoder(batch)\n",
    "        loss = criterion(batch, mu, log_var, reconstruction)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses_per_epoch.append(loss.item())\n",
    "\n",
    "    train_losses.append(np.mean(train_losses_per_epoch))\n",
    "\n",
    "    autoencoder.eval()\n",
    "    val_losses_per_epoch = [] \n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            batch = batch.float().to(device)\n",
    "            reconstruction, mu, log_var = autoencoder(batch)\n",
    "            loss = criterion(batch, mu, log_var, reconstruction)\n",
    "            val_losses_per_epoch.append(loss.item())\n",
    "\n",
    "    val_losses.append(np.mean(val_losses_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(np.arange(len(train_losses)), train_losses, label='Train')\n",
    "plt.plot(np.arange(len(val_losses)), val_losses, label='Validation')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('MSE loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAztAMA4YY7q"
   },
   "source": [
    "Давайте посмотрим, как наш тренированный автоэнкодер кодирует и восстанавливает картинки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1J__yvxYY7r"
   },
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "  reconstruction, mu, log_var = autoencoder(batch.to(device))\n",
    "  reconstruction = reconstruction\n",
    "  result = reconstruction.cpu().detach().numpy().transpose(0, 2, 3, 1)\n",
    "  ground_truth = batch.cpu().numpy().transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 20))\n",
    "for i, (gt, res) in enumerate(zip(ground_truth[:5], result[:5])):\n",
    "  plt.subplot(5, 2, 2*i+1)\n",
    "  plt.imshow(gt)\n",
    "  plt.subplot(5, 2, 2*i+2)\n",
    "  plt.imshow(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1OPh9O6UYY7s"
   },
   "source": [
    "Not bad, right? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFi96giuYY7t"
   },
   "source": [
    "## 1.4. Sampling (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOtUaPNYYY7t"
   },
   "source": [
    "Давайте теперь будем не просто брать картинку, прогонять ее через автоэекодер и получать реконструкцию, а попробуем создать что-то НОВОЕ\n",
    "\n",
    "Давайте возьмем и подсунем декодеру какие-нибудь сгенерированные нами векторы (например, из нормального распределения) и посмотрим на результат реконструкции декодера:\n",
    "\n",
    "__Подсказка:__Е сли вместо лиц у вас выводится непонятно что, попробуйте посмотреть, как выглядят латентные векторы картинок из датасета. Так как в обучении нейронных сетей есть определенная доля рандома, векторы латентного слоя могут быть распределены НЕ как `np.random.randn(25, <latent_space_dim>)`. А чтобы у нас получались лица при запихивании вектора декодеру, вектор должен быть распределен так же, как латентные векторы реальных фоток. Так что в таком случае придется рандом немного подогнать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8IZykARRYY7u",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# сгенерируем 25 рандомных векторов размера latent_space\n",
    "z = np.random.normal(loc=0, scale=1, size=(25, dim_code))\n",
    "#z = np.random.randn(25, dim_code)\n",
    "output = autoencoder.decode(torch.FloatTensor(z).to(device))\n",
    "\n",
    "output = output.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(18, 18))\n",
    "for i in range(output.shape[0]):\n",
    "  plt.subplot(output.shape[0] // 5, 5, i + 1)\n",
    "  generated = output[i]\n",
    "  plt.imshow((generated * 255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ey8dD9s0YY7w"
   },
   "source": [
    "## Time to make fun! (4 балла)\n",
    "\n",
    "Давайте научимся пририсовывать людям улыбки =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1v-8WwuYY7w"
   },
   "source": [
    "<img src=\"https://i.imgur.com/tOE9rDK.png\" alt=\"linear\" width=\"700\" height=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGE0M2GDYY7x"
   },
   "source": [
    "План такой:\n",
    "\n",
    "1. Нужно выделить \"вектор улыбки\": для этого нужно из выборки изображений найти несколько (~15) людей с улыбками и столько же без.\n",
    "\n",
    "Найти людей с улыбками вам поможет файл с описанием датасета, скачанный вместе с датасетом. В нем указаны имена картинок и присутствубщие атрибуты (улыбки, очки...)\n",
    "\n",
    "2. Вычислить латентный вектор для всех улыбающихся людей (прогнать их через encoder) и то же для всех грустненьких\n",
    "\n",
    "3. Вычислить, собственно, вектор улыбки -- посчитать разность между средним латентным вектором улыбающихся людей и средним латентным вектором грустных людей\n",
    "\n",
    "4. А теперь приделаем улыбку грустному человеку: добавим полученный в пункте 3 вектор к латентному вектору грустного человека и прогоним полученный вектор через decoder. Получим того же человека, но уже не грустненького!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "smiling = all_attrs.index[all_attrs['Smiling'] > 2.5][:25].values\n",
    "\n",
    "for idx, i in enumerate(smiling):\n",
    "  plt.subplot(5, 5, idx + 1)\n",
    "  generated = my_dataset[i].permute(1, 2, 0)\n",
    "  plt.imshow(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 18))\n",
    "sad = all_attrs.index[all_attrs['Frowning'] > 2.5][:25].values\n",
    "for idx, i in enumerate(sad):\n",
    "  plt.subplot(5, 5, idx + 1)\n",
    "  generated = my_dataset[i].permute(1, 2, 0)\n",
    "  plt.imshow(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1oBX9EeYY7x"
   },
   "outputs": [],
   "source": [
    "sum_smile = torch.zeros_like(torch.zeros((1, dim_code))).to(device)\n",
    "\n",
    "for smile_idx in smiling:\n",
    "    smiling_single = my_dataset[smile_idx].unsqueeze(0).to(device)\n",
    "\n",
    "    latent = autoencoder.encode(smiling_single)\n",
    "\n",
    "    reconstructed = autoencoder.get_latent_vector(*latent)\n",
    "\n",
    "    sum_smile += reconstructed\n",
    "\n",
    "smile_vector = sum_smile.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sad = torch.zeros_like(torch.zeros((1, dim_code))).to(device)\n",
    "\n",
    "for sad_idx in sad:\n",
    "    sad_single = my_dataset[sad_idx].unsqueeze(0).to(device)\n",
    "\n",
    "    latent = autoencoder.encode(sad_single)\n",
    "\n",
    "    reconstructed = autoencoder.get_latent_vector(*latent)\n",
    "\n",
    "    sum_sad += reconstructed\n",
    "\n",
    "sad_vector = sum_sad.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([my_dataset[sad_idx] for sad_idx in sad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sad_dataset = Dataset([my_dataset[sad_idx] for sad_idx in sad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = smile_vector - sad_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    reconstruction, mu, log_var = autoencoder(sad_dataset.permute(0, 2, 3, 1))\n",
    "    latent = autoencoder.get_latent_vector(mu, log_var) + delta\n",
    "    \n",
    "    reconstruction_smile = autoencoder.decode(latent)\n",
    "    reconstruction_smile = reconstruction_smile.view(-1, 64, 64, 3)\n",
    "    reconstruction_smile = reconstruction_smile.cpu().detach().numpy()\n",
    "    \n",
    "    reconstruction = reconstruction.view(-1, 64, 64, 3)\n",
    "    result = reconstruction.cpu().detach().numpy()\n",
    "    \n",
    "    ground_truth = sad_dataset.cpu().numpy().transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5, 50))\n",
    "for i, (gt, res, sm) in enumerate(zip(ground_truth[:5], result[:5], reconstruction_smile[:5])):\n",
    "  plt.subplot(25, 3, 3*i+1)\n",
    "  plt.axis('off')\n",
    "  plt.imshow(gt, interpolation='nearest')\n",
    "  \n",
    "  plt.subplot(25, 3, 3*i+2)\n",
    "  plt.axis('off')\n",
    "  plt.imshow(res, interpolation='nearest')\n",
    "  \n",
    "  plt.subplot(25, 3, 3*i+3)\n",
    "  plt.axis('off')\n",
    "  plt.imshow(sm, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = autoencoder.decode(delta.unsqueeze(0).to(device))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i in range(output.shape[0]):\n",
    "  plt.subplot(1, 1, i + 1)\n",
    "  generated = output[i].cpu().detach().numpy().transpose(1, 2, 0)\n",
    "  plt.imshow(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Это вектор улыбки (decoded) &#8593; &#8593; &#8593;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXI6jprOYY7z"
   },
   "source": [
    "Вуаля! Вы восхитительны!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2UAf0bpYY70"
   },
   "source": [
    "Теперь вы можете пририсовывать людям не только улыбки, но и много чего другого -- закрывать/открывать глаза, пририсовывать очки... в общем, все, на что хватит фантазии и на что есть атрибуты в `all_attrs` :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQnEGmknYY71"
   },
   "source": [
    "# Часть 2: Variational Autoencoder (10 баллов) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bWQNRjJq2uTz"
   },
   "source": [
    "Займемся обучением вариационных автоэнкодеров — проапгрейженной версии AE. Обучать будем на датасете MNIST, содержащем написанные от руки цифры от 0 до 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBXXr9njByYC"
   },
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "# MNIST Dataset\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rHphW5l8Wgi"
   },
   "source": [
    "## 2.1 Архитектура модели и обучение (2 балла)\n",
    "\n",
    "Реализуем VAE. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Рекомендуем пользоваться более сложными моделями, чем та, что была на семинаре :) Экспериментируйте!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IoNVT5tYYY74"
   },
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "    \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7*7*32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, dim_code * 2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim_code, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32*7*7),\n",
    "            nn.Unflatten(1, torch.Size([32, 7, 7])),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x).view(-1, 2, dim_code)\n",
    "        \n",
    "        mu = x[:, 0, :]\n",
    "        logsigma = x[:, 1, :]\n",
    "        \n",
    "        return mu, logsigma\n",
    "    \n",
    "    def gaussian_sampler(self, mu, logsigma):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logsigma)\n",
    "            eps = torch.randn_like(std)\n",
    "            sample = mu + (eps * std)\n",
    "            return sample\n",
    "        else:\n",
    "            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n",
    "            # на инференсе выход автоэнкодера должен быть детерминирован.\n",
    "            return mu\n",
    "    \n",
    "    def decode(self, z):       \n",
    "        x = self.decoder(z)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logsigma = self.encode(x)\n",
    "        \n",
    "        latent = self.gaussian_sampler(mu, logsigma)\n",
    "            \n",
    "        reconstruction = self.decode(latent)\n",
    "        reconstruction = torch.sigmoid(reconstruction)\n",
    "        \n",
    "        return reconstruction, mu, logsigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAB77d-PYY76"
   },
   "source": [
    "Определим лосс и его компоненты для VAE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UxJrkXGQo5bp"
   },
   "source": [
    "Надеюсь, вы уже прочитали материал в towardsdatascience (или еще где-то) про VAE и знаете, что лосс у VAE состоит из двух частей: KL и log-likelihood.\n",
    "\n",
    "Общий лосс будет выглядеть так:\n",
    "\n",
    "$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n",
    "\n",
    "Формула для KL-дивергенции:\n",
    "\n",
    "$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n",
    "\n",
    "В качестве log-likelihood возьмем привычную нам кросс-энтропию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ac5ey7uIYY77"
   },
   "outputs": [],
   "source": [
    "def KL_divergence(mu, logsigma):\n",
    "    loss = -0.5 * torch.sum(1 + logsigma - mu.pow(2) - logsigma.exp())\n",
    "    return loss\n",
    "\n",
    "def log_likelihood(x, reconstruction):\n",
    "    loss = nn.BCELoss(reduction='sum')\n",
    "    return loss(reconstruction, x)\n",
    "\n",
    "def loss_vae(x, mu, logsigma, reconstruction):\n",
    "    return KL_divergence(mu, logsigma) + log_likelihood(x, reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPJQu70eYY79"
   },
   "source": [
    "И обучим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtCjfqXdYY79"
   },
   "outputs": [],
   "source": [
    "criterion = loss_vae\n",
    "\n",
    "autoencoder = VAE().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY1khca6YY7_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "n_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in trange(n_epochs):\n",
    "    autoencoder.train()\n",
    "    train_losses_per_epoch = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logsigma = autoencoder(X_batch.to(device))\n",
    "        reconstruction = reconstruction.view(-1, 1, 28, 28)\n",
    "        loss = criterion(X_batch.to(device).float(), mu, logsigma, reconstruction)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses_per_epoch.append(loss.item())\n",
    "\n",
    "    train_losses.append(np.mean(train_losses_per_epoch))\n",
    "\n",
    "    autoencoder.eval()\n",
    "    val_losses_per_epoch = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "          reconstruction, mu, logsigma = autoencoder(X_batch.to(device))\n",
    "          reconstruction = reconstruction.view(-1, 1, 28, 28)\n",
    "          loss = criterion(X_batch.to(device).float(), mu, logsigma, reconstruction)\n",
    "          val_losses_per_epoch.append(loss.item())\n",
    "\n",
    "    val_losses.append(np.mean(val_losses_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(np.arange(len(train_losses)), train_losses, label='Train')\n",
    "plt.plot(np.arange(len(val_losses)), val_losses, label='Validation')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('MSE loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkxW_8fkYY8B"
   },
   "source": [
    "Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Jd3BWM_YY8C"
   },
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        reconstruction, mu, logsigma = autoencoder(X_batch.to(device))\n",
    "        reconstruction = reconstruction.view(-1, 28, 28, 1)\n",
    "        result = reconstruction.cpu().detach().numpy()\n",
    "        ground_truth = X_batch.numpy().transpose(0, 2, 3, 1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 20))\n",
    "for i, (gt, res) in enumerate(zip(ground_truth[:5], result[:5])):\n",
    "  plt.subplot(5, 2, 2*i+1)\n",
    "  plt.imshow(gt)\n",
    "  plt.subplot(5, 2, 2*i+2)\n",
    "  plt.imshow(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQXYIXjoYY8F"
   },
   "source": [
    "Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOhhH-osYY8G"
   },
   "outputs": [],
   "source": [
    "# вспомните про замечание из этого же пункта обычного AE про распределение латентных переменных\n",
    "z = np.array([np.random.normal(0, 1, 100) for i in range(10)])\n",
    "output = <скормите z декодеру>\n",
    "<выведите тут полученные картинки>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzt-ENxCr6ul"
   },
   "source": [
    "## 2.2. Latent Representation (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIWy670xr-Uv"
   },
   "source": [
    "Давайте посмотрим, как латентные векторы картинок цифр выглядят в пространстве.\n",
    "Ваша задача -- изобразить латентные векторы картинок точками в двумерном просторанстве. \n",
    "\n",
    "Это позволит оценить, насколько плотно распределены латентные векторы изображений цифр в пространстве. \n",
    "\n",
    "Плюс давайте сделаем такую вещь: покрасим точки, которые соответствуют картинкам каждой цифры, в свой отдельный цвет\n",
    "\n",
    "Подсказка: красить -- это просто =) У plt.scatter есть параметр c (color), см. в документации.\n",
    "\n",
    "\n",
    "Итак, план:\n",
    "1. Получить латентные представления картинок тестового датасета\n",
    "2. С помощтю `TSNE` (есть в `sklearn`) сжать эти представления до размерности 2 (чтобы можно было их визуализировать точками в пространстве)\n",
    "3. Визуализировать полученные двумерные представления с помощью `matplotlib.scatter`, покрасить разными цветами точки, соответствующие картинкам разных цифр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "      reconstruction, mu, logsigma = autoencoder(X_batch.to(device))\n",
    "      \n",
    "      X.extend(autoencoder.gaussian_sampler(mu, logsigma).detach().cpu().numpy())\n",
    "      y.extend(y_batch.detach().cpu().numpy())\n",
    "      \n",
    "      reconstruction = reconstruction.view(-1, 28, 28, 1)\n",
    "      result = reconstruction.cpu().detach().numpy()\n",
    "      ground_truth = X_batch.numpy().transpose(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "\n",
    "tsne = TSNE()\n",
    "\n",
    "X_embedded = tsne.fit_transform(X)\n",
    "\n",
    "palette = sns.color_palette(\"bright\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=y, legend='full', palette=palette);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifxhsvPss5h_"
   },
   "source": [
    "Что вы думаете о виде латентного представления?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESPBHrL3YY8H"
   },
   "source": [
    "__Congrats v2.0!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yIYuKFwijN2U"
   },
   "source": [
    "## 2.3. Conditional VAE (6 баллов)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5l8Bu1RPjUx"
   },
   "source": [
    "Мы уже научились обучать обычный AE на датасете картинок и получать новые картинки, используя генерацию шума и декодер. \n",
    "Давайте теперь допустим, что мы обучили AE на датасете MNIST и теперь хотим генерировать новые картинки с числами с помощью декодера (как выше мы генерили рандомные лица). \n",
    "И вот нам понадобилось сгенерировать цифру 8, и мы подставляем разные варианты шума, но восьмерка никак не генерится:(\n",
    "\n",
    "Хотелось бы добавить к нашему AE функцию \"выдай мне рандомное число из вот этого вот класса\", где классов десять (цифры от 0 до 9 образуют десять классов).  Conditional AE — так называется вид автоэнкодера, который предоставляет такую возможность. Ну, название \"conditional\" уже говорит само за себя.\n",
    "\n",
    "И в этой части задания мы научимся такие обучать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0j8zNIwKPY-6"
   },
   "source": [
    "### Архитектура\n",
    "\n",
    "На картинке ниже представлена архитектура простого Conditional VAE.\n",
    "\n",
    "По сути, единственное отличие от обычного -- это то, что мы вместе с картинкой в первом слое энкодера и декодера передаем еще информацию о классе картинки. \n",
    "\n",
    "То есть, в первый (входной) слой энкодера подается конкатенация картинки и информации о классе (например, вектора из девяти нулей и одной единицы). В первый слой декодера подается конкатенация латентного вектора и информации о классе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6YloFEAPeM4"
   },
   "source": [
    "\n",
    "![alt text](https://sun9-63.userapi.com/impg/Mh1akf7mfpNoprrSWsPOouazSmTPMazYYF49Tw/djoHNw_9KVA.jpg?size=1175x642&quality=96&sign=e88baec5f9bb91c8443fba31dcf0a4df&type=album)\n",
    "\n",
    "![alt text](https://sun9-73.userapi.com/impg/UDuloLNKhzTBYAKewgxke5-YPsAKyGOqA-qCRg/MnyCavJidxM.jpg?size=1229x651&quality=96&sign=f2d21bfacc1c5755b76868dc4cfef39c&type=album)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxg2tDSfRbLF"
   },
   "source": [
    "На всякий случай: это VAE, то есть, latent у него все еще состоит из mu и sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpFbSXLaPrm1"
   },
   "source": [
    "Таким образом, при генерации новой рандомной картинки мы должны будем передать декодеру сконкатенированные латентный вектор и класс картинки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX0zxklMPwI2"
   },
   "source": [
    "P.S. Также можно передавать класс картинки не только в первый слой, но и в каждый слой сети. То есть на каждом слое конкатенировать выход из предыдущего слоя и информацию о классе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ar701cHOkDKS"
   },
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CVAE, self).__init__()\n",
    "    \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7*7*32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, dim_code * 2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dim_code, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32*7*7),\n",
    "            nn.Unflatten(1, torch.Size([32, 7, 7])),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Conv2d(16, 1, kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def encode(self, x, class_num):\n",
    "        x = self.encoder(x).view(-1, 2, dim_code)\n",
    "        \n",
    "        mu = x[:, 0, :]\n",
    "        logsigma = x[:, 1, :]\n",
    "        \n",
    "        return mu, logsigma, class_num\n",
    "    \n",
    "    def gaussian_sampler(self, mu, logsigma):\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logsigma)\n",
    "            eps = torch.randn_like(std)\n",
    "            sample = mu + (eps * std)\n",
    "            return sample\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def decode(self, z):       \n",
    "        x = self.decoder(z)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, class_num):\n",
    "        print(x.shape)\n",
    "        print(class_num)\n",
    "        \n",
    "        mu, logsigma = self.encode(x, class_num)\n",
    "        \n",
    "        latent = self.gaussian_sampler(mu, logsigma)\n",
    "            \n",
    "        reconstruction = self.decode(latent)\n",
    "        reconstruction = torch.sigmoid(reconstruction)\n",
    "        \n",
    "        return reconstruction, mu, logsigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtCjfqXdYY79"
   },
   "outputs": [],
   "source": [
    "criterion = loss_vae\n",
    "\n",
    "autoencoder = CVAE().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY1khca6YY7_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "n_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in trange(n_epochs):\n",
    "    autoencoder.train()\n",
    "    train_losses_per_epoch = []\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logsigma = autoencoder(X_batch.to(device), y_batch)\n",
    "        reconstruction = reconstruction.view(-1, 1, 28, 28)\n",
    "        loss = criterion(X_batch.to(device).float(), mu, logsigma, reconstruction)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses_per_epoch.append(loss.item())\n",
    "\n",
    "    train_losses.append(np.mean(train_losses_per_epoch))\n",
    "\n",
    "    autoencoder.eval()\n",
    "    val_losses_per_epoch = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "          reconstruction, mu, logsigma = autoencoder(X_batch.to(device), y_batch)\n",
    "          reconstruction = reconstruction.view(-1, 1, 28, 28)\n",
    "          loss = criterion(X_batch.to(device).float(), mu, logsigma, reconstruction)\n",
    "          val_losses_per_epoch.append(loss.item())\n",
    "\n",
    "    val_losses.append(np.mean(val_losses_per_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(np.arange(len(train_losses)), train_losses, label='Train')\n",
    "plt.plot(np.arange(len(val_losses)), val_losses, label='Validation')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('MSE loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Jd3BWM_YY8C"
   },
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        reconstruction, mu, logsigma = autoencoder(X_batch.to(device))\n",
    "        reconstruction = reconstruction.view(-1, 28, 28, 1)\n",
    "        result = reconstruction.cpu().detach().numpy()\n",
    "        ground_truth = X_batch.numpy().transpose(0, 2, 3, 1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 20))\n",
    "for i, (gt, res) in enumerate(zip(ground_truth[:5], result[:5])):\n",
    "  plt.subplot(5, 2, 2*i+1)\n",
    "  plt.imshow(gt)\n",
    "  plt.subplot(5, 2, 2*i+2)\n",
    "  plt.imshow(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoMw-IFyP5A2"
   },
   "source": [
    "### Sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe1zWyZHkLV2"
   },
   "source": [
    "Тут мы будем сэмплировать из CVAE. Это прикольнее, чем сэмплировать из простого AE/VAE: тут можно взять один и тот же латентный вектор и попросить CVAE восстановить из него картинки разных классов!\n",
    "Для MNIST вы можете попросить CVAE восстановить из одного латентного вектора, например, картинки цифры 5 и 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A0SQIhvNP9Dr"
   },
   "outputs": [],
   "source": [
    "<тут нужно научиться сэмплировать из декодера цифры определенного класса>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAWBu8rzQBgQ"
   },
   "source": [
    "Splendid! Вы великолепны!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt2S77cm3O1v"
   },
   "source": [
    "### Latent Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nt7x8Ek_rHTE"
   },
   "source": [
    "Давайте посмотрим, как выглядит латентное пространство картинок в CVAE и сравним с картинкой для VAE =)\n",
    "\n",
    "Опять же, нужно покрасить точки в разные цвета в зависимости от класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LSCYK7sH3KEc"
   },
   "outputs": [],
   "source": [
    "<ваш код получения латентных представлений, применения TSNE и визуализации>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET8IELWu3Z2c"
   },
   "source": [
    "Что вы думаете насчет этой картинки? Отличается от картинки для VAE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWkqHjvTCD_8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KN3D_k5W_WZz"
   },
   "source": [
    "# BONUS 1: Denoising\n",
    "\n",
    "## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12a1jkpkCsIU"
   },
   "source": [
    "У автоэнкодеров, кроме сжатия и генерации изображений, есть другие практические применения. Про одно из них эта бонусная часть задания."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v8EN-8jlCtmd"
   },
   "source": [
    "Автоэнкодеры могут быть использованы для избавления от шума на фотографиях (denoising). Для этого их нужно обучить специальным образом: input картинка будет зашумленной, а выдавать автоэнкодер должен будет картинку без шума. \n",
    "То есть, loss-функция AE останется той же (MSE между реальной картинкой и выданной), а на вход автоэнкодеру будет подаваться зашумленная картинка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1OJg6jhlaZl"
   },
   "source": [
    "<a href=\"https://ibb.co/YbRJ1nZ\"><img src=\"https://i.ibb.co/0QD164t/Screen-Shot-2020-06-04-at-4-49-50-PM.png\" alt=\"Screen-Shot-2020-06-04-at-4-49-50-PM\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysI0BCuRDbvm"
   },
   "source": [
    "Для этого нужно взять ваш любимый датасет (датасет лиц из первой части этого задания или любой другой) и сделать копию этого датасета с шумом. \n",
    "\n",
    "В питоне шум можно добавить так:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5e746iVDgSm"
   },
   "outputs": [],
   "source": [
    "noise_factor = 0.5\n",
    "X_noisy = X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fSPkXMtDpd5"
   },
   "outputs": [],
   "source": [
    "<тут ваш код обучения автоэнкодера на зашумленных картинках. Не забудтье разбить на train/test!>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B03NQ_sKDvg2"
   },
   "outputs": [],
   "source": [
    "<тут проверка, как AE убирает щум с тестовых картинок. Надеюсь, все получилось =)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NDiCPYLm2bY"
   },
   "source": [
    "# BONUS 2: Image Retrieval\n",
    "\n",
    "## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xao_27WMm7AL"
   },
   "source": [
    "Давайте представим, что весь наш тренировочный датасет -- это большая база данных людей. И вот мы получили картинку лица какого-то человека с уличной камеры наблюдения (у нас это картинка из тестового датасета) и хотим понять, что это за человек. Что нам делать? Правильно -- берем наш VAE, кодируем картинку в латентное представление и ищем среди латентныз представлений лиц нашей базы самые ближайшие!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y__bdS23ndeY"
   },
   "source": [
    "План:\n",
    "\n",
    "1. Получаем латентные представления всех лиц тренировочного датасета\n",
    "2. Обучаем на них LSHForest `(sklearn.neighbors.LSHForest)`, например, с `n_estimators=50`\n",
    "3. Берем картинку из тестового датасета, с помощью VAE получаем ее латентный вектор\n",
    "4. Ищем с помощью обученного LSHForest ближайшие из латентных представлений тренировочной базы\n",
    "5. Находим лица тренировочного датасета, которым соответствуют ближайшие латентные представления, визуализируем!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IksC2ucIoND-"
   },
   "source": [
    "Немного кода вам в помощь: (feel free to delete everything and write your own)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hK0YpLMRoEa0"
   },
   "outputs": [],
   "source": [
    "codes = <поучите латентные представления картинок из трейна>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KisDrgZdoWdt"
   },
   "outputs": [],
   "source": [
    "# обучаем LSHForest\n",
    "from sklearn.neighbors import LSHForest\n",
    "lshf = LSHForest(n_estimators=50).fit(codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_S5zPb5obam"
   },
   "outputs": [],
   "source": [
    "def get_similar(image, n_neighbors=5):\n",
    "  # функция, которая берет тестовый image и с помощью метода kneighbours у LSHForest ищет ближайшие векторы\n",
    "  # прогоняет векторы через декодер и получает картинки ближайших людей\n",
    "\n",
    "  code = <получение латентного представления image>\n",
    "    \n",
    "  (distances,),(idx,) = lshf.kneighbors(code, n_neighbors=n_neighbors)\n",
    "\n",
    "  return distances, X_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2kjV5wupLP_"
   },
   "outputs": [],
   "source": [
    "def show_similar(image):\n",
    "\n",
    "  # функция, которая принимает тестовый image, ищет ближайшие к нему и визуализирует результат\n",
    "    \n",
    "    distances,neighbors = get_similar(image,n_neighbors=11)\n",
    "    \n",
    "    plt.figure(figsize=[8,6])\n",
    "    plt.subplot(3,4,1)\n",
    "    plt.imshow(image.cpu().numpy().transpose([1,2,0]))\n",
    "    plt.title(\"Original image\")\n",
    "    \n",
    "    for i in range(11):\n",
    "        plt.subplot(3,4,i+2)\n",
    "        plt.imshow(neighbors[i].cpu().numpy().transpose([1,2,0]))\n",
    "        plt.title(\"Dist=%.3f\"%distances[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3Ja1UNf_oJq"
   },
   "outputs": [],
   "source": [
    "<тут выведите самые похожие лица к какому-нибудь лицу из тестовой части датасета>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "dFi96giuYY7t",
    "Ey8dD9s0YY7w",
    "KN3D_k5W_WZz",
    "-NDiCPYLm2bY"
   ],
   "name": "[hw]autoencoders.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}